---
- hosts: "{{master.name}}"
  remote_user: "{{master.user}}"
  gather_facts: false
  vars:
    # required extra vars:
    #   - host
    #   - task_name
    #   - task_id
    #   - task_rep
    ansible_sudo_pass: "{{master.password}}"
    db_name: Clusters
    apt_requirements:
      # - libcurl4
    deploy_home: "/home/{{master.user}}/Desktop/code/{{db_name}}/HiBench"
    deploy_flink_home: "/home/{{master.user}}/Desktop/code/{{db_name}}/flink/flink-1.10.0"
    deploy_spark_home: "/home/{{master.user}}/Desktop/code/{{db_name}}/spark/spark-2.2.2-bin-hadoop2.7"
    deploy_hadoop_home:  "/home/{{master.user}}/Desktop/code/{{db_name}}/hadoop/hadoop-2.7.7"
    local_benchmark_src: ../HiBench.zip
    benchmark_src: "{{deploy_home}}/HiBench.zip"
    db_server: "{{deploy_home}}/HiBench"
    local_flink_conf: ../conf/Hibench/flink.conf
    local_spark_conf: ../conf/Hibench/spark.conf
    local_hadoop_conf: ../conf/Hibench/hadoop.conf
    local_hibench_conf: ../conf/Hibench/hibench.conf
    local_hibench_fixwindow_conf: ../conf/Hibench/workloads/fixwindow.conf
    db_config: "{{db_server}}/conf/"
    db_port: 9092
    flink_port: 8082
    spark_port: 8080
    run_results_bin: "{{db_server}}/results"
    collect_results: "{{db_server}}/report"
    local_result_dir: "../results/{{task_name}}"
  pre_tasks: # set up a clean env
    - name: load app_config information
      include_vars:
        file: "{{local_result_dir}}/{{task_id}}_app_config.yml"
        name: app_config
    - name: load cloud_config information
      include_vars:
        file: "{{local_result_dir}}/{{task_id}}_cloud_config.yml"
        name: cloud_config
    - name: create folders
      with_items:
        - "{{deploy_home}}"
        - "{{run_results_bin}}"
        - "{{collect_results}}"
      file:
        path: "{{item}}"
        state: directory
        recurse: yes
    - name: copy HiBench archive
      copy:
        src: "{{local_benchmark_src}}"
        dest: "{{benchmark_src}}"
      when: env == "vcenter"
    - name: unarchive HiBench
      unarchive:
        src: "{{benchmark_src}}"
        dest: "{{deploy_home}}"
        remote_src: yes
      when: env == "vcenter"
  tasks:
    - name: copy flink conf
      template:
        src: "{{local_flink_conf}}"
        dest: "{{db_config}}"
      when: objective == "flink"
    - name: copy spark conf
      template:
        src: "{{local_spark_conf}}"
        dest: "{{db_config}}"
      when: objective == "spark"
    - name: copy hadoop conf
      template:
        src: "{{local_hadoop_conf}}"
        dest: "{{db_config}}"
    - name: copy hibench conf
      template:
        src: "{{local_hibench_conf}}"
        dest: "{{db_config}}"
    - name: copy fixwindow conf
      template:
        src: "{{local_hibench_fixwindow_conf}}"
        dest: "{{db_server}}/conf/workloads/streaming/"
      when: workload == "fixwindow"
    - name: clear report
      file:
        path: "{{db_server}}/report"
        state: "{{item}}"
      with_items:
        - absent
        - directory
    - name: start spark
      shell: "JAVA_HOME=/usr/local/jdk nohup {{deploy_spark_home}}/sbin/start-all.sh"
      when: objective == "spark"
    - name: wait...
      wait_for:
        host: "{{master.ip}}"
        port: "{{spark_port}}"
        delay: 5 # wait 3 seconds till it initialized?
      when: objective == "spark"
# 换成离线上传数据
    - name: generate data
      shell: "JAVA_HOME=/usr/local/jdk nohup {{db_server}}/bin/workloads/streaming/{{workload}}/prepare/genSeedDataset.sh"
      async : 300
      poll: 2
    - name: start flink
      shell: "JAVA_HOME=/usr/local/jdk nohup {{deploy_flink_home}}/bin/start-cluster.sh"
      when: objective == "flink"
    - name: wait...
      wait_for:
        host: "{{master.ip}}"
        port: "{{flink_port}}"
        delay: 5 # wait 3 seconds till it initialized?
      when: objective == "flink"
    - name: send data
      shell: "JAVA_HOME=/usr/local/jdk nohup {{db_server}}/bin/workloads/streaming/{{workload}}/prepare/dataGen.sh -s 1>{{run_results_bin}}/runResult1 2>{{run_results_bin}}/runError1"
      async: 300
      poll: 0
    - name: sending wait ....
      shell: sleep 5
    - name: consume data
      shell: "JAVA_HOME=/usr/local/jdk nohup {{db_server}}/bin/workloads/streaming/{{workload}}/{{objective}}/run.sh -s 1>{{run_results_bin}}/runResult 2>{{run_results_bin}}/runError"
      async: 300
      poll: 0
    - name: testing wait ....
      shell: sleep 310
      async : 310
      poll: 2
    - name: fetch send data result
      fetch:
        src: "{{run_results_bin}}/runResult1"
        dest: "{{local_result_dir}}/{{task_id}}_send_data_{{task_rep}}_result"
        flat: yes
    - name: fetch send data error
      fetch:
        src: "{{run_results_bin}}/runError1"
        dest: "{{local_result_dir}}/{{task_id}}_send_data_{{task_rep}}_error"
        flat: yes
    - name: fetch consume data result
      fetch:
        src: "{{run_results_bin}}/runResult"
        dest: "{{local_result_dir}}/{{task_id}}_consume_data_{{task_rep}}_result"
        flat: yes
    - name: fetch consume data error
      fetch:
        src: "{{run_results_bin}}/runError"
        dest: "{{local_result_dir}}/{{task_id}}_consume_data_{{task_rep}}_error"
        flat: yes
    - name: stop flink
      shell: "JAVA_HOME=/usr/local/jdk nohup {{deploy_flink_home}}/bin/stop-cluster.sh"
      when: objective == "flink"
    - name: chmod 777 metrics_reader.sh
      shell: chmod 777 {{db_server}}/bin/workloads/streaming/{{workload}}/common/metrics_reader.sh;
    - name: collect metrics
      shell: kafka_topic="$(grep -o '{{objective |upper}}_{{workload}}[0-9_]\+' {{run_results_bin}}/runResult | tail -1)";JAVA_HOME=/usr/local/jdk {{db_server}}/bin/workloads/streaming/{{workload}}/common/metrics_reader.sh $kafka_topic -s 1>{{run_results_bin}}/collectResult 2>{{run_results_bin}}/collectError;mv {{db_server}}/report/$kafka_topic.csv {{db_server}}/report/test.csv;
      args:
        executable: /bin/bash
      register: result
      until: result.rc == 0
      retries: 5
      delay: 10
    - name: stop spark
      shell: "JAVA_HOME=/usr/local/jdk nohup {{deploy_spark_home}}/sbin/stop-all.sh"
      when: objective == "spark"
    - name: fetch collect data result
      fetch:
        src: "{{run_results_bin}}/collectResult"
        dest: "{{local_result_dir}}/{{task_id}}_collect_{{task_rep}}_result"
        flat: yes
    - name: fetch collect data error
      fetch:
        src: "{{run_results_bin}}/collectError"
        dest: "{{local_result_dir}}/{{task_id}}_collect_{{task_rep}}_error"
        flat: yes
    - name: fetch run result
      fetch:
        src: "{{db_server}}/report/test.csv"
        dest: "{{local_result_dir}}/{{task_id}}_run_result_{{task_rep}}.csv"
        flat: yes
